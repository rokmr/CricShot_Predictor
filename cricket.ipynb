{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import subprocess\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Device Agnostic \n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ffmpeg located at: /opt/homebrew/bin/ffmpeg\n"
     ]
    }
   ],
   "source": [
    "#Setting up FFMPEG PATH\n",
    "FFMPEG_PATH = shutil.which(\"ffmpeg\")\n",
    "\n",
    "if FFMPEG_PATH is None:\n",
    "    print(\"Error: ffmpeg not found\")\n",
    "else:\n",
    "    print(f\"ffmpeg located at: {FFMPEG_PATH}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter the URL of dataset and the folder name\n",
    "url = \"https://drive.usercontent.google.com/download?id=1yR1GONn37dwKg8jU-5Zi_rCDNmdC2k4L&export=download&authuser=0&confirm=t&uuid=7a4b81a1-ada3-4540-9c3d-fec60be47fa1&at=APZUnTU3AzOQ5iOSa7qQ6b-gRJGQ%3A1714239992461\"\n",
    "zip_path = \"./cricketshot.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] File doesn't exist, downloading...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1.41G/1.41G [03:28<00:00, 6.73MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] The file has been downloaded and saved as ./cricketshot.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def download_file_with_progress(url, save_path):\n",
    "    if not os.path.exists(save_path):\n",
    "        print(\"[INFO] File doesn't exist, downloading...\")\n",
    "\n",
    "        response = requests.get(url, stream=True)\n",
    "        total_size = int(response.headers.get('content-length', 0))\n",
    "        chunk_size = 1024  # Adjust as needed\n",
    "\n",
    "        progress_bar = tqdm(total=total_size, unit='B', unit_scale=True)\n",
    "\n",
    "        with open(save_path, \"wb\") as file:\n",
    "            for chunk in response.iter_content(chunk_size=chunk_size):\n",
    "                file.write(chunk)\n",
    "                progress_bar.update(len(chunk))\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        if total_size != 0 and progress_bar.n != total_size:\n",
    "            print(\"[INFO] Error downloading the file. Please check the URL.\")\n",
    "        else:\n",
    "            print(f\"[INFO] The file has been downloaded and saved as {save_path}\")\n",
    "    else:\n",
    "        print(f\"[INFO] File {save_path} exists.\")\n",
    "\n",
    "\n",
    "download_file_with_progress(url, zip_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO]: Downloaded folder have been unzipped!\n",
      "Current working directory: /Users/rohitkumar/Documents/fanplayiot_final/cricketshot\n"
     ]
    }
   ],
   "source": [
    "# Unzip the folder\n",
    "current_path = os.getcwd()\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(current_path)\n",
    "\n",
    "print(\"[INFO]: Downloaded folder have been unzipped!\")\n",
    "os.chdir(\"./cricketshot\")\n",
    "current_path = os.getcwd()\n",
    "print(\"Current working directory:\", current_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base path where your class folders are located\n",
    "base_path = \"./CricShot10dataset\"\n",
    "\n",
    "# Output directory to save frames\n",
    "output_dir = \"./frames\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed lofted_0198.avi in lofted\n",
      "Processed lofted_0173.avi in lofted\n",
      "Processed lofted_0167.avi in lofted\n",
      "Processed lofted_0007.avi in lofted\n",
      "Processed lofted_0013.avi in lofted\n",
      "Processed lofted_0012.avi in lofted\n",
      "Processed lofted_0006.avi in lofted\n",
      "Processed lofted_0166.avi in lofted\n",
      "Processed lofted_0172.avi in lofted\n",
      "Processed lofted_0164.avi in lofted\n",
      "Processed lofted_0170.avi in lofted\n",
      "Processed lofted_0010.avi in lofted\n",
      "Processed lofted_0004.avi in lofted\n",
      "Processed lofted_0038.avi in lofted\n",
      "Processed lofted_0039.avi in lofted\n",
      "Processed lofted_0005.avi in lofted\n",
      "Processed lofted_0011.avi in lofted\n",
      "Processed lofted_0171.avi in lofted\n",
      "Processed lofted_0165.avi in lofted\n",
      "Processed lofted_0161.avi in lofted\n",
      "Processed lofted_0175.avi in lofted\n",
      "Processed lofted_0029.avi in lofted\n",
      "Processed lofted_0015.avi in lofted\n",
      "Processed lofted_0001.avi in lofted\n",
      "Processed lofted_0014.avi in lofted\n",
      "Processed lofted_0028.avi in lofted\n",
      "Processed lofted_0174.avi in lofted\n",
      "Processed lofted_0148.avi in lofted\n",
      "Processed lofted_0176.avi in lofted\n",
      "Processed lofted_0162.avi in lofted\n",
      "Processed lofted_0002.avi in lofted\n",
      "Processed lofted_0016.avi in lofted\n",
      "Processed lofted_0017.avi in lofted\n",
      "Processed lofted_0003.avi in lofted\n",
      "Processed lofted_0163.avi in lofted\n",
      "Processed lofted_0177.avi in lofted\n",
      "Processed lofted_0110.avi in lofted\n",
      "Processed lofted_0104.avi in lofted\n",
      "Processed lofted_0064.avi in lofted\n",
      "Processed lofted_0058.avi in lofted\n",
      "Processed lofted_0059.avi in lofted\n",
      "Processed lofted_0071.avi in lofted\n",
      "Processed lofted_0065.avi in lofted\n",
      "Processed lofted_0105.avi in lofted\n",
      "Processed lofted_0111.avi in lofted\n",
      "Processed lofted_0107.avi in lofted\n",
      "Processed lofted_0113.avi in lofted\n",
      "Processed lofted_0067.avi in lofted\n",
      "Processed lofted_0099.avi in lofted\n",
      "Processed lofted_0112.avi in lofted\n",
      "Processed lofted_0106.avi in lofted\n",
      "Processed lofted_0089.avi in lofted\n",
      "Processed lofted_0062.avi in lofted\n",
      "Processed lofted_0063.avi in lofted\n",
      "Processed lofted_0088.avi in lofted\n",
      "Processed lofted_0103.avi in lofted\n",
      "Processed lofted_0049.avi in lofted\n",
      "Processed lofted_0061.avi in lofted\n",
      "Processed lofted_0060.avi in lofted\n",
      "Processed lofted_0048.avi in lofted\n",
      "Processed lofted_0114.avi in lofted\n",
      "Processed lofted_0086.avi in lofted\n",
      "Processed lofted_0045.avi in lofted\n",
      "Processed lofted_0051.avi in lofted\n",
      "Processed lofted_0050.avi in lofted\n",
      "Processed lofted_0044.avi in lofted\n",
      "Processed lofted_0087.avi in lofted\n",
      "Processed lofted_0091.avi in lofted\n",
      "Processed lofted_0085.avi in lofted\n",
      "Processed lofted_0052.avi in lofted\n",
      "Processed lofted_0046.avi in lofted\n",
      "Processed lofted_0047.avi in lofted\n",
      "Processed lofted_0053.avi in lofted\n",
      "Processed lofted_0084.avi in lofted\n",
      "Processed lofted_0090.avi in lofted\n",
      "Processed lofted_0057.avi in lofted\n",
      "Processed lofted_0043.avi in lofted\n",
      "Processed lofted_0042.avi in lofted\n",
      "Processed lofted_0056.avi in lofted\n",
      "Processed lofted_0120.avi in lofted\n",
      "Processed lofted_0040.avi in lofted\n",
      "Processed lofted_0054.avi in lofted\n",
      "Processed lofted_0055.avi in lofted\n",
      "Processed lofted_0041.avi in lofted\n",
      "Processed lofted_0109.avi in lofted\n",
      "Processed lofted_0026.avi in lofted\n",
      "Processed lofted_0032.avi in lofted\n",
      "Processed lofted_0033.avi in lofted\n",
      "Processed lofted_0027.avi in lofted\n",
      "Processed lofted_0186.avi in lofted\n",
      "Processed lofted_0179.avi in lofted\n",
      "Processed lofted_0031.avi in lofted\n",
      "Processed lofted_0025.avi in lofted\n",
      "Processed lofted_0019.avi in lofted\n",
      "Processed lofted_0018.avi in lofted\n",
      "Processed lofted_0024.avi in lofted\n",
      "Processed lofted_0030.avi in lofted\n",
      "Processed lofted_0178.avi in lofted\n",
      "Processed lofted_0193.avi in lofted\n",
      "Processed lofted_0197.avi in lofted\n",
      "Processed lofted_0168.avi in lofted\n",
      "Processed lofted_0008.avi in lofted\n",
      "Processed lofted_0034.avi in lofted\n",
      "Processed lofted_0020.avi in lofted\n",
      "Processed lofted_0021.avi in lofted\n",
      "Processed lofted_0035.avi in lofted\n",
      "Processed lofted_0009.avi in lofted\n",
      "Processed lofted_0169.avi in lofted\n",
      "Processed lofted_0196.avi in lofted\n",
      "Processed lofted_0194.avi in lofted\n",
      "Processed lofted_0023.avi in lofted\n",
      "Processed lofted_0037.avi in lofted\n",
      "Processed lofted_0036.avi in lofted\n",
      "Processed lofted_0022.avi in lofted\n",
      "Processed lofted_0195.avi in lofted\n",
      "Processed sweep_0088.avi in sweep\n",
      "Processed sweep_0103.avi in sweep\n",
      "Processed sweep_0117.avi in sweep\n",
      "Processed sweep_0116.avi in sweep\n",
      "Processed sweep_0102.avi in sweep\n",
      "Processed sweep_0089.avi in sweep\n",
      "Processed sweep_0048.avi in sweep\n",
      "Processed sweep_0114.avi in sweep\n",
      "Processed sweep_0100.avi in sweep\n",
      "Processed sweep_0128.avi in sweep\n",
      "Processed sweep_0129.avi in sweep\n",
      "Processed sweep_0101.avi in sweep\n",
      "Processed sweep_0115.avi in sweep\n",
      "Processed sweep_0049.avi in sweep\n",
      "Processed sweep_0075.avi in sweep\n",
      "Processed sweep_0139.avi in sweep\n",
      "Processed sweep_0111.avi in sweep\n",
      "Processed sweep_0105.avi in sweep\n",
      "Processed sweep_0110.avi in sweep\n",
      "Processed sweep_0099.avi in sweep\n",
      "Processed sweep_0106.avi in sweep\n",
      "Processed sweep_0112.avi in sweep\n",
      "Processed sweep_0113.avi in sweep\n",
      "Processed sweep_0107.avi in sweep\n",
      "Processed sweep_0098.avi in sweep\n",
      "Processed sweep_0028.avi in sweep\n",
      "Processed sweep_0160.avi in sweep\n",
      "Processed sweep_0174.avi in sweep\n",
      "Processed sweep_0148.avi in sweep\n",
      "Processed sweep_0149.avi in sweep\n",
      "Processed sweep_0175.avi in sweep\n",
      "Processed sweep_0161.avi in sweep\n",
      "Processed sweep_0029.avi in sweep\n",
      "Processed sweep_0015.avi in sweep\n",
      "Processed sweep_0003.avi in sweep\n",
      "Processed sweep_0017.avi in sweep\n",
      "Processed sweep_0177.avi in sweep\n",
      "Processed sweep_0163.avi in sweep\n",
      "Processed sweep_0188.avi in sweep\n",
      "Processed sweep_0189.avi in sweep\n",
      "Processed sweep_0162.avi in sweep\n",
      "Processed sweep_0176.avi in sweep\n",
      "Processed sweep_0016.avi in sweep\n",
      "Processed sweep_0002.avi in sweep\n",
      "Processed sweep_0006.avi in sweep\n",
      "Processed sweep_0012.avi in sweep\n",
      "Processed sweep_0172.avi in sweep\n",
      "Processed sweep_0166.avi in sweep\n",
      "Processed sweep_0167.avi in sweep\n",
      "Processed sweep_0173.avi in sweep\n",
      "Processed sweep_0007.avi in sweep\n",
      "Processed sweep_0039.avi in sweep\n",
      "Processed sweep_0005.avi in sweep\n",
      "Processed sweep_0159.avi in sweep\n",
      "Processed sweep_0165.avi in sweep\n",
      "Processed sweep_0171.avi in sweep\n",
      "Processed sweep_0170.avi in sweep\n",
      "Processed sweep_0164.avi in sweep\n",
      "Processed sweep_0158.avi in sweep\n",
      "Processed sweep_0004.avi in sweep\n",
      "Processed sweep_0038.avi in sweep\n",
      "Processed sweep_0035.avi in sweep\n",
      "Processed sweep_0021.avi in sweep\n",
      "Processed sweep_0009.avi in sweep\n",
      "Processed sweep_0141.avi in sweep\n",
      "Processed sweep_0155.avi in sweep\n",
      "Processed sweep_0169.avi in sweep\n",
      "Processed sweep_0182.avi in sweep\n",
      "Processed sweep_0183.avi in sweep\n",
      "Processed sweep_0168.avi in sweep\n",
      "Processed sweep_0154.avi in sweep\n",
      "Processed sweep_0140.avi in sweep\n",
      "Processed sweep_0008.avi in sweep\n",
      "Processed sweep_0020.avi in sweep\n",
      "Processed sweep_0034.avi in sweep\n",
      "Processed sweep_0022.avi in sweep\n",
      "Processed sweep_0036.avi in sweep\n",
      "Processed sweep_0156.avi in sweep\n",
      "Processed sweep_0142.avi in sweep\n",
      "Processed sweep_0181.avi in sweep\n",
      "Processed sweep_0180.avi in sweep\n",
      "Processed sweep_0194.avi in sweep\n",
      "Processed sweep_0143.avi in sweep\n",
      "Processed sweep_0157.avi in sweep\n",
      "Processed sweep_0037.avi in sweep\n",
      "Processed sweep_0027.avi in sweep\n",
      "Processed sweep_0033.avi in sweep\n",
      "Processed sweep_0153.avi in sweep\n",
      "Processed sweep_0147.avi in sweep\n",
      "Processed sweep_0190.avi in sweep\n",
      "Processed sweep_0184.avi in sweep\n",
      "Processed sweep_0185.avi in sweep\n",
      "Processed sweep_0191.avi in sweep\n",
      "Processed sweep_0146.avi in sweep\n",
      "Processed sweep_0152.avi in sweep\n",
      "Processed sweep_0032.avi in sweep\n",
      "Processed sweep_0026.avi in sweep\n",
      "Processed sweep_0018.avi in sweep\n",
      "Processed sweep_0030.avi in sweep\n",
      "Processed sweep_0024.avi in sweep\n",
      "Processed sweep_0178.avi in sweep\n",
      "Processed sweep_0144.avi in sweep\n",
      "Processed sweep_0150.avi in sweep\n",
      "Processed sweep_0187.avi in sweep\n",
      "Processed sweep_0193.avi in sweep\n",
      "Processed sweep_0192.avi in sweep\n",
      "Processed sweep_0186.avi in sweep\n",
      "Processed sweep_0151.avi in sweep\n",
      "Processed sweep_0145.avi in sweep\n",
      "Processed sweep_0179.avi in sweep\n",
      "Processed sweep_0025.avi in sweep\n",
      "Processed sweep_0031.avi in sweep\n",
      "Processed sweep_0019.avi in sweep\n",
      "Processed sweep_0042.avi in sweep\n",
      "Processed sweep_0095.avi in sweep\n",
      "Processed sweep_0081.avi in sweep\n",
      "Processed sweep_0122.avi in sweep\n",
      "Processed sweep_0136.avi in sweep\n",
      "Processed sweep_0137.avi in sweep\n",
      "Processed sweep_0123.avi in sweep\n",
      "Processed sweep_0080.avi in sweep\n",
      "Processed sweep_0094.avi in sweep\n",
      "Processed sweep_0043.avi in sweep\n",
      "Processed sweep_0041.avi in sweep\n",
      "Processed sweep_0082.avi in sweep\n",
      "Processed sweep_0096.avi in sweep\n",
      "Processed sweep_0135.avi in sweep\n",
      "Processed sweep_0121.avi in sweep\n",
      "Processed sweep_0109.avi in sweep\n",
      "Processed sweep_0108.avi in sweep\n",
      "Processed sweep_0120.avi in sweep\n",
      "Processed sweep_0040.avi in sweep\n",
      "Processed sweep_0044.avi in sweep\n",
      "Processed sweep_0050.avi in sweep\n",
      "Processed sweep_0087.avi in sweep\n",
      "Processed sweep_0118.avi in sweep\n",
      "Processed sweep_0130.avi in sweep\n",
      "Processed sweep_0124.avi in sweep\n",
      "Processed sweep_0125.avi in sweep\n",
      "Processed sweep_0131.avi in sweep\n",
      "Processed sweep_0119.avi in sweep\n",
      "Processed sweep_0086.avi in sweep\n",
      "Processed sweep_0051.avi in sweep\n",
      "Processed sweep_0045.avi in sweep\n",
      "Processed sweep_0079.avi in sweep\n",
      "Processed sweep_0053.avi in sweep\n",
      "Processed sweep_0047.avi in sweep\n",
      "Processed sweep_0084.avi in sweep\n",
      "Processed sweep_0127.avi in sweep\n",
      "Processed sweep_0133.avi in sweep\n",
      "Processed sweep_0132.avi in sweep\n",
      "Processed sweep_0126.avi in sweep\n",
      "Processed sweep_0085.avi in sweep\n",
      "Processed sweep_0091.avi in sweep\n",
      "Processed sweep_0046.avi in sweep\n",
      "Processed sweep_0052.avi in sweep\n",
      "Processed square_cut_0039.avi in square_cut\n",
      "Processed square_cut_0038.avi in square_cut\n",
      "Processed square_cut_0004.avi in square_cut\n",
      "Processed square_cut_0010.avi in square_cut\n",
      "Processed square_cut_0166.avi in square_cut\n",
      "Processed square_cut_0002.avi in square_cut\n",
      "Processed square_cut_0001.avi in square_cut\n",
      "Processed square_cut_0072.avi in square_cut\n",
      "Processed square_cut_0067.avi in square_cut\n",
      "Processed square_cut_0073.avi in square_cut\n",
      "Processed square_cut_0111.avi in square_cut\n",
      "Processed square_cut_0071.avi in square_cut\n",
      "Processed square_cut_0059.avi in square_cut\n",
      "Processed square_cut_0070.avi in square_cut\n",
      "Processed square_cut_0138.avi in square_cut\n",
      "Processed square_cut_0074.avi in square_cut\n",
      "Processed square_cut_0049.avi in square_cut\n",
      "Processed square_cut_0101.avi in square_cut\n",
      "Processed square_cut_0077.avi in square_cut\n",
      "Processed square_cut_0076.avi in square_cut\n",
      "Processed square_cut_0130.avi in square_cut\n",
      "Processed square_cut_0078.avi in square_cut\n",
      "Processed square_cut_0069.avi in square_cut\n",
      "Processed square_cut_0068.avi in square_cut\n",
      "Processed square_cut_0030.avi in square_cut\n",
      "Processed square_cut_0024.avi in square_cut\n",
      "Processed square_cut_0025.avi in square_cut\n",
      "Processed square_cut_0031.avi in square_cut\n",
      "Processed square_cut_0027.avi in square_cut\n",
      "Processed square_cut_0033.avi in square_cut\n",
      "Processed square_cut_0032.avi in square_cut\n",
      "Processed square_cut_0026.avi in square_cut\n",
      "Processed square_cut_0036.avi in square_cut\n",
      "Processed square_cut_0035.avi in square_cut\n",
      "Processed square_cut_0034.avi in square_cut\n",
      "Processed square_cut_0008.avi in square_cut\n",
      "Processed defense_0144.avi in defense\n",
      "Processed defense_0030.avi in defense\n",
      "Processed defense_0145.avi in defense\n",
      "Processed defense_0033.avi in defense\n",
      "Processed defense_0026.avi in defense\n",
      "Processed defense_0146.avi in defense\n",
      "Processed defense_0152.avi in defense\n",
      "Processed defense_0142.avi in defense\n",
      "Processed defense_0036.avi in defense\n",
      "Processed defense_0037.avi in defense\n",
      "Processed defense_0023.avi in defense\n",
      "Processed defense_0143.avi in defense\n",
      "Processed defense_0141.avi in defense\n",
      "Processed defense_0035.avi in defense\n",
      "Processed defense_0020.avi in defense\n",
      "Processed defense_0034.avi in defense\n",
      "Processed defense_0140.avi in defense\n",
      "Processed defense_0133.avi in defense\n",
      "Processed defense_0084.avi in defense\n",
      "Processed defense_0053.avi in defense\n",
      "Processed defense_0047.avi in defense\n",
      "Processed defense_0046.avi in defense\n",
      "Processed defense_0052.avi in defense\n",
      "Processed defense_0132.avi in defense\n",
      "Processed defense_0130.avi in defense\n",
      "Processed defense_0078.avi in defense\n",
      "Processed defense_0044.avi in defense\n",
      "Processed defense_0050.avi in defense\n",
      "Processed defense_0051.avi in defense\n",
      "Processed defense_0045.avi in defense\n",
      "Processed defense_0079.avi in defense\n",
      "Processed defense_0092.avi in defense\n",
      "Processed defense_0131.avi in defense\n",
      "Processed defense_0135.avi in defense\n",
      "Processed defense_0121.avi in defense\n",
      "Processed defense_0109.avi in defense\n",
      "Processed defense_0082.avi in defense\n",
      "Processed defense_0041.avi in defense\n",
      "Processed defense_0055.avi in defense\n",
      "Processed defense_0069.avi in defense\n",
      "Processed defense_0068.avi in defense\n",
      "Processed defense_0054.avi in defense\n",
      "Processed defense_0040.avi in defense\n",
      "Processed defense_0083.avi in defense\n",
      "Processed defense_0108.avi in defense\n",
      "Processed defense_0134.avi in defense\n",
      "Processed defense_0136.avi in defense\n",
      "Processed defense_0081.avi in defense\n",
      "Processed defense_0056.avi in defense\n",
      "Processed defense_0042.avi in defense\n",
      "Processed defense_0043.avi in defense\n",
      "Processed defense_0057.avi in defense\n",
      "Processed defense_0080.avi in defense\n",
      "Processed defense_0137.avi in defense\n",
      "Processed defense_0106.avi in defense\n",
      "Processed defense_0112.avi in defense\n",
      "Processed defense_0072.avi in defense\n",
      "Processed defense_0066.avi in defense\n",
      "Processed defense_0067.avi in defense\n",
      "Processed defense_0098.avi in defense\n",
      "Processed defense_0113.avi in defense\n",
      "Processed defense_0107.avi in defense\n",
      "Processed defense_0139.avi in defense\n",
      "Processed defense_0111.avi in defense\n",
      "Processed defense_0105.avi in defense\n",
      "Processed defense_0059.avi in defense\n",
      "Processed defense_0065.avi in defense\n",
      "Processed defense_0071.avi in defense\n",
      "Processed defense_0070.avi in defense\n",
      "Processed defense_0064.avi in defense\n",
      "Processed defense_0058.avi in defense\n",
      "Processed defense_0104.avi in defense\n",
      "Processed defense_0110.avi in defense\n",
      "Processed defense_0138.avi in defense\n",
      "Processed defense_0060.avi in defense\n",
      "Processed defense_0048.avi in defense\n",
      "Processed defense_0049.avi in defense\n",
      "Processed defense_0075.avi in defense\n",
      "Processed defense_0061.avi in defense\n",
      "Processed defense_0115.avi in defense\n",
      "Processed defense_0103.avi in defense\n",
      "Processed defense_0117.avi in defense\n",
      "Processed defense_0077.avi in defense\n",
      "Processed defense_0063.avi in defense\n",
      "Processed defense_0062.avi in defense\n",
      "Processed defense_0076.avi in defense\n",
      "Processed defense_0116.avi in defense\n",
      "Processed defense_0159.avi in defense\n",
      "Processed defense_0165.avi in defense\n",
      "Processed defense_0039.avi in defense\n",
      "Processed defense_0038.avi in defense\n",
      "Processed defense_0164.avi in defense\n",
      "Processed defense_0013.avi in defense\n",
      "Processed defense_0007.avi in defense\n",
      "Processed defense_0163.avi in defense\n",
      "Processed defense_0002.avi in defense\n",
      "Processed defense_0162.avi in defense\n",
      "Processed defense_0160.avi in defense\n",
      "Processed defense_0028.avi in defense\n",
      "Processed defense_0001.avi in defense\n",
      "Processed defense_0015.avi in defense\n",
      "Processed defense_0161.avi in defense\n"
     ]
    }
   ],
   "source": [
    "# Iterate over each class folder\n",
    "for class_folder in os.listdir(base_path):\n",
    "    class_folder_path = os.path.join(base_path, class_folder)\n",
    "    \n",
    "    # Ensure it's a directory\n",
    "    if os.path.isdir(class_folder_path):\n",
    "        # Create an output folder for each class\n",
    "        class_output_dir = os.path.join(output_dir, class_folder)\n",
    "        os.makedirs(class_output_dir, exist_ok=True)\n",
    "\n",
    "        # Iterate over each video file in the class folder\n",
    "        for video_file in os.listdir(class_folder_path):\n",
    "            video_path = os.path.join(class_folder_path, video_file)\n",
    "            video_name = os.path.splitext(video_file)[0]\n",
    "            output_pattern = os.path.join(class_output_dir, f\"{video_name}_%04d.jpg\")\n",
    "            \n",
    "            # Command to extract frames at 1 fps\n",
    "            ffmpeg_command = f\"{FFMPEG_PATH} -i {video_path} -vf fps=1 {output_pattern} -loglevel quiet\"\n",
    "            \n",
    "            # Execute the command\n",
    "            try:\n",
    "                subprocess.run(ffmpeg_command, shell=True, check=True)\n",
    "                print(f\"Processed {video_file} in {class_folder}\")\n",
    "            except subprocess.CalledProcessError:\n",
    "                print(f\"Failed to process video: {video_file} in {class_folder}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1/4 subfolders.\n",
      "Processed 2/4 subfolders.\n",
      "Processed 3/4 subfolders.\n",
      "Processed 4/4 subfolders.\n",
      "All subfolders processed.\n"
     ]
    }
   ],
   "source": [
    "# frames embeddings creation \n",
    "\n",
    "# Path to the model and processor directories\n",
    "saved_model_path = './model'\n",
    "saved_processor_path = './processor'\n",
    "\n",
    "# Load the CLIP model and processor\n",
    "processor = CLIPProcessor.from_pretrained(saved_processor_path)\n",
    "clip_model = CLIPModel.from_pretrained(saved_model_path)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model.to(device)\n",
    "\n",
    "def batch_process_images(image_paths, batch_size, processor, model, device):\n",
    "    embeddings = []\n",
    "    for i in range(0, len(image_paths), batch_size):\n",
    "        batch_paths = image_paths[i:i + batch_size]\n",
    "        batch_images = [Image.open(path).convert(\"RGB\") for path in batch_paths]\n",
    "        tokens = processor(\n",
    "            text=None,\n",
    "            images=batch_images,\n",
    "            return_tensors=\"pt\"\n",
    "        ).to(device)\n",
    "        batch_embeddings = model.get_image_features(**tokens)\n",
    "        batch_embeddings = batch_embeddings.detach().cpu().numpy()\n",
    "        embeddings.append(batch_embeddings)\n",
    "    return np.concatenate(embeddings, axis=0)\n",
    "\n",
    "# Path where the extracted frames are stored\n",
    "main_folder = './frames'\n",
    "output_folder = './frames_embeddings'\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "subfolders = [f.path for f in os.scandir(main_folder) if f.is_dir()]\n",
    "total_subfolders = len(subfolders)\n",
    "processed_subfolders = 0\n",
    "\n",
    "for subfolder in subfolders:\n",
    "    subfolder_name = os.path.basename(subfolder)\n",
    "    output_subfolder = os.path.join(output_folder, subfolder_name)\n",
    "    os.makedirs(output_subfolder, exist_ok=True)\n",
    "    image_files = [f for f in os.listdir(subfolder) if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    \n",
    "    if image_files:\n",
    "        image_paths = [os.path.join(subfolder, f) for f in image_files]\n",
    "        embeddings = batch_process_images(image_paths, batch_size=100, processor=processor, model=clip_model, device=device)\n",
    "\n",
    "        # Save each embedding with a filename that reflects its original image\n",
    "        for i, emb in enumerate(embeddings):\n",
    "            original_file_name = image_files[i].rsplit('.', 1)[0]  # Remove extension\n",
    "            output_path = os.path.join(output_subfolder, f'{original_file_name}_embedding.npy')\n",
    "            np.save(output_path, emb)\n",
    "\n",
    "    processed_subfolders += 1\n",
    "    print(f\"Processed {processed_subfolders}/{total_subfolders} subfolders.\")\n",
    "\n",
    "print(\"All subfolders processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and their labels\n",
    "def load_embeddings_and_labels(embeddings_folder):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    label_mapping = {}  # To convert class names to numerical labels\n",
    "    current_label = 0\n",
    "\n",
    "    for class_folder in sorted(os.listdir(embeddings_folder)):\n",
    "        class_path = os.path.join(embeddings_folder, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            if class_folder not in label_mapping:\n",
    "                label_mapping[class_folder] = current_label\n",
    "                current_label += 1\n",
    "            for emb_file in sorted(os.listdir(class_path)):\n",
    "                if emb_file.endswith('_embedding.npy'):\n",
    "                    emb_path = os.path.join(class_path, emb_file)\n",
    "                    embeddings.append(np.load(emb_path))\n",
    "                    labels.append(label_mapping[class_folder])\n",
    "\n",
    "    embeddings = torch.tensor(embeddings, dtype=torch.float32)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return embeddings, labels, label_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the LSTM neural network\n",
    "class LSTMNetwork(nn.Module):\n",
    "    def __init__(self, input_size=512, hidden_size=256, num_classes=4):\n",
    "        super(LSTMNetwork, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.fc(x[:, -1, :])  # Use the output of the last time step\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xm/hv0x78kx5mvbzd2tsn4q98km0000gn/T/ipykernel_66326/1934974694.py:20: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1712608659634/work/torch/csrc/utils/tensor_new.cpp:277.)\n",
      "  embeddings = torch.tensor(embeddings, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "embeddings_folder = './frames_embeddings'\n",
    "os.makedirs(embeddings_folder, exist_ok=True)\n",
    "\n",
    "embeddings, labels, class_label_mapping = load_embeddings_and_labels(embeddings_folder)\n",
    "\n",
    "# Split data\n",
    "dataset = TensorDataset(embeddings.unsqueeze(1), labels)  # Add an extra dimension for LSTM sequence length\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = int(0.15 * len(dataset))\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Model\n",
    "model = LSTMNetwork(input_size=512, hidden_size=256, num_classes=len(class_label_mapping)).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 52.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Loss: 1.148882295936346\n",
      "Validation Accuracy: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 76.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Training Loss: 0.7554335594177246\n",
      "Validation Accuracy: 0.67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 60.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Training Loss: 0.6354547329246998\n",
      "Validation Accuracy: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 34.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Training Loss: 0.5643118508160114\n",
      "Validation Accuracy: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 39.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Training Loss: 0.5163781829178333\n",
      "Validation Accuracy: 0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 30.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Training Loss: 0.44454110972583294\n",
      "Validation Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 73.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Training Loss: 0.39107582066208124\n",
      "Validation Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 68.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Training Loss: 0.39299500547349453\n",
      "Validation Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 75.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Training Loss: 0.31345116812735796\n",
      "Validation Accuracy: 0.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 76.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Training Loss: 0.28965521790087223\n",
      "Validation Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 70.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Training Loss: 0.2615962466225028\n",
      "Validation Accuracy: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 55.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Training Loss: 0.24786584917455912\n",
      "Validation Accuracy: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 43.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Training Loss: 0.2222776673734188\n",
      "Validation Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 42.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Training Loss: 0.19434516178444028\n",
      "Validation Accuracy: 0.75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 69.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Training Loss: 0.20066893054172397\n",
      "Validation Accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 65.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Training Loss: 0.17452391795814037\n",
      "Validation Accuracy: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 74.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Training Loss: 0.179461270570755\n",
      "Validation Accuracy: 0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 31.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Training Loss: 0.15086807357147336\n",
      "Validation Accuracy: 0.77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 53.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Training Loss: 0.13932127389125526\n",
      "Validation Accuracy: 0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16/16 [00:00<00:00, 74.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Training Loss: 0.1372630875557661\n",
      "Validation Accuracy: 0.78\n",
      "Test Accuracy: 0.87\n",
      "Confusion Matrix:\n",
      "[[58  1  0  0]\n",
      " [ 0 79  1  7]\n",
      " [ 0  2  7  3]\n",
      " [ 2 11  1 38]]\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     defense       0.97      0.98      0.97        59\n",
      "      lofted       0.85      0.91      0.88        87\n",
      "  square_cut       0.78      0.58      0.67        12\n",
      "       sweep       0.79      0.73      0.76        52\n",
      "\n",
      "    accuracy                           0.87       210\n",
      "   macro avg       0.85      0.80      0.82       210\n",
      "weighted avg       0.86      0.87      0.86       210\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data, target in tqdm(train_loader, desc=f'Epoch {epoch + 1}'):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Training Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    total_correct = total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total_correct += (predicted == target).sum().item()\n",
    "            total += target.size(0)\n",
    "\n",
    "    val_accuracy = total_correct / total\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.2f}\")\n",
    "\n",
    "# Test\n",
    "model.eval()\n",
    "total_correct = total = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total_correct += (predicted == target).sum().item()\n",
    "        total += target.size(0)\n",
    "\n",
    "test_accuracy = total_correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "\n",
    "#confusion matrix\n",
    "all_labels = []\n",
    "all_preds = []\n",
    "for data, target in test_loader:\n",
    "    data, target = data.to(device), target.to(device)\n",
    "    output = model(data)\n",
    "    _, predicted = torch.max(output, 1)\n",
    "    all_labels.extend(target.tolist())\n",
    "    all_preds.extend(predicted.tolist())\n",
    "\n",
    "conf_mat = confusion_matrix(all_labels, all_preds)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_mat)\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(all_labels, all_preds, target_names=list(class_label_mapping.keys()))\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "filepath = \"./cricket.pt\"\n",
    "torch.save(model.state_dict(), filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx_to_class = {v:k for k,v in class_label_mapping.items()}\n",
    "# FFMPEG_PATH = \"/opt/homebrew/bin/ffmpeg\"\n",
    "# input_file = \"/Users/rohitkumar/Documents/fanplayiot/demo/defense_0001.avi\"\n",
    "# frames_dir = \"/Users/rohitkumar/Documents/fanplayiot/demo/frames/\"\n",
    "# os.makedirs(frames_dir, exist_ok=True)\n",
    "\n",
    "# video_name = os.path.splitext(input_file.split(\"/\")[-1])[-2]\n",
    "# output_pattern = os.path.join(frames_dir, f\"{video_name}_%04d.jpg\")\n",
    "# ffmpeg_command = f\"{FFMPEG_PATH} -i {input_file} -vf fps=1 {output_pattern} -loglevel quiet\"\n",
    "\n",
    "# try:\n",
    "#     subprocess.run(ffmpeg_command, shell=True, check=True)\n",
    "#     print(f\"Processed {input_file}.\")\n",
    "# except subprocess.CalledProcessError:\n",
    "#     print(f\"Failed to process video: {input_file}\")\n",
    "\n",
    "# image_paths = []\n",
    "# for root, dirs, files in os.walk(frames_dir):\n",
    "#     for file in files:\n",
    "#         if file.lower().endswith(('.png', '.jpg', '.jpeg', '.gif', '.bmp')):\n",
    "#             image_paths.append(os.path.join(root, file))\n",
    "            \n",
    "# inference_images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
    "# tokens = processor(text=None, images=inference_images, return_tensors=\"pt\").to(device)\n",
    "# inference_embeddings = clip_model.get_image_features(**tokens)\n",
    "# with torch.no_grad():\n",
    "#     output = model(inference_embeddings.unsqueeze(0))\n",
    "#     idx = output.argmax()\n",
    "#     print(idx_to_class[idx.item()])\n",
    "\n",
    "# try:\n",
    "#     # Attempt to delete the folder and its contents\n",
    "#     shutil.rmtree(frames_dir)\n",
    "#     print(f\"Folder '{frames_dir}' and its contents have been deleted.\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error while deleting folder '{frames_dir}': {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fanplayiot",
   "language": "python",
   "name": "fanplayiot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
